#!/usr/bin/env python3
"""
Multi-Scale Attention Modules - Anatomical Complexity Mitigation

Implements advanced attention mechanisms to handle complex anatomical structures
in embryonic brainstem segmentation with varying scales and morphologies.

Key Features:
- Spatial attention across multiple scales
- Channel attention for feature refinement
- Morphogen-guided attention weighting
- Hierarchical anatomical structure modeling
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class AttentionConfig:
    """Configuration for multi-scale attention modules."""
    
    # Spatial attention
    spatial_scales: List[int] = None  # [1, 2, 4, 8] - multi-scale receptive fields
    spatial_reduction_ratio: int = 8
    
    # Channel attention  
    channel_reduction_ratio: int = 16
    
    # Morphogen attention
    morphogen_channels: int = 3  # SHH, BMP, WNT
    morphogen_weight: float = 0.3
    
    # Hierarchical attention
    hierarchy_levels: int = 3  # subdivision -> nucleus -> sub-nucleus
    
    def __post_init__(self):
        if self.spatial_scales is None:
            self.spatial_scales = [1, 2, 4, 8]


class SpatialAttentionModule(nn.Module):
    """Multi-scale spatial attention for anatomical structure focus."""
    
    def __init__(self, in_channels: int, config: AttentionConfig):
        super().__init__()
        self.config = config
        self.scales = config.spatial_scales
        
        # Multi-scale convolutions
        self.scale_convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv3d(in_channels, in_channels // config.spatial_reduction_ratio, 
                         kernel_size=3, padding=scale, dilation=scale, bias=False),
                nn.BatchNorm3d(in_channels // config.spatial_reduction_ratio),
                nn.ReLU(inplace=True)
            ) for scale in self.scales
        ])
        
        # Attention fusion
        reduced_channels = (in_channels // config.spatial_reduction_ratio) * len(self.scales)
        self.fusion_conv = nn.Sequential(
            nn.Conv3d(reduced_channels, in_channels // 4, kernel_size=1, bias=False),
            nn.BatchNorm3d(in_channels // 4),
            nn.ReLU(inplace=True),
            nn.Conv3d(in_channels // 4, 1, kernel_size=1, bias=False),
            nn.Sigmoid()
        )
        
        # Global context
        self.global_pool = nn.AdaptiveAvgPool3d(1)
        self.global_fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // config.spatial_reduction_ratio),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // config.spatial_reduction_ratio, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Apply multi-scale spatial attention.
        
        Args:
            x: Input tensor [B, C, H, W, D]
            
        Returns:
            Attention-weighted tensor [B, C, H, W, D]
        """
        B, C, H, W, D = x.shape
        
        # Multi-scale feature extraction
        scale_features = []
        for scale_conv in self.scale_convs:
            scale_feat = scale_conv(x)
            scale_features.append(scale_feat)
        
        # Concatenate multi-scale features
        multi_scale = torch.cat(scale_features, dim=1)  # [B, C_reduced*scales, H, W, D]
        
        # Generate spatial attention map
        spatial_attention = self.fusion_conv(multi_scale)  # [B, 1, H, W, D]
        
        # Global context attention
        global_feat = self.global_pool(x).view(B, C)  # [B, C]
        global_attention = self.global_fc(global_feat).view(B, 1, 1, 1, 1)  # [B, 1, 1, 1, 1]
        
        # Combine spatial and global attention
        combined_attention = spatial_attention * global_attention
        
        # Apply attention
        attended_x = x * combined_attention
        
        return attended_x


class ChannelAttentionModule(nn.Module):
    """Channel attention for feature refinement and selection."""
    
    def __init__(self, in_channels: int, config: AttentionConfig):
        super().__init__()
        self.config = config
        
        # Global average and max pooling
        self.global_avg_pool = nn.AdaptiveAvgPool3d(1)
        self.global_max_pool = nn.AdaptiveMaxPool3d(1)
        
        # Shared MLP
        reduced_channels = max(1, in_channels // config.channel_reduction_ratio)
        self.shared_mlp = nn.Sequential(
            nn.Linear(in_channels, reduced_channels),
            nn.ReLU(inplace=True),
            nn.Linear(reduced_channels, in_channels)
        )
        
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Apply channel attention.
        
        Args:
            x: Input tensor [B, C, H, W, D]
            
        Returns:
            Channel-attended tensor [B, C, H, W, D]
        """
        B, C, H, W, D = x.shape
        
        # Global pooling
        avg_pool = self.global_avg_pool(x).view(B, C)  # [B, C]
        max_pool = self.global_max_pool(x).view(B, C)  # [B, C]
        
        # Shared MLP
        avg_attention = self.shared_mlp(avg_pool)  # [B, C]
        max_attention = self.shared_mlp(max_pool)  # [B, C]
        
        # Combine and apply sigmoid
        channel_attention = self.sigmoid(avg_attention + max_attention)  # [B, C]
        channel_attention = channel_attention.view(B, C, 1, 1, 1)  # [B, C, 1, 1, 1]
        
        # Apply attention
        attended_x = x * channel_attention
        
        return attended_x


class MorphogenGuidedAttention(nn.Module):
    """Morphogen-guided attention for biologically-informed feature weighting."""
    
    def __init__(self, in_channels: int, config: AttentionConfig):
        super().__init__()
        self.config = config
        self.morphogen_channels = config.morphogen_channels
        self.morphogen_weight = config.morphogen_weight
        
        # Morphogen processing
        self.morphogen_conv = nn.Sequential(
            nn.Conv3d(self.morphogen_channels, 16, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm3d(16),
            nn.ReLU(inplace=True),
            nn.Conv3d(16, 8, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm3d(8),
            nn.ReLU(inplace=True)
        )
        
        # Feature-morphogen fusion
        self.fusion_conv = nn.Sequential(
            nn.Conv3d(in_channels + 8, in_channels // 2, kernel_size=1, bias=False),
            nn.BatchNorm3d(in_channels // 2),
            nn.ReLU(inplace=True),
            nn.Conv3d(in_channels // 2, in_channels, kernel_size=1, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor, morphogens: torch.Tensor) -> torch.Tensor:
        """
        Apply morphogen-guided attention.
        
        Args:
            x: Input features [B, C, H, W, D]
            morphogens: Morphogen gradients [B, 3, H, W, D]
            
        Returns:
            Morphogen-guided attended features [B, C, H, W, D]
        """
        # Process morphogen gradients
        morphogen_features = self.morphogen_conv(morphogens)  # [B, 8, H, W, D]
        
        # Concatenate features and morphogens
        combined = torch.cat([x, morphogen_features], dim=1)  # [B, C+8, H, W, D]
        
        # Generate attention weights
        attention_weights = self.fusion_conv(combined)  # [B, C, H, W, D]
        
        # Apply weighted attention
        base_attention = torch.ones_like(attention_weights)
        morphogen_attention = (1 - self.morphogen_weight) * base_attention + self.morphogen_weight * attention_weights
        
        attended_x = x * morphogen_attention
        
        return attended_x


class HierarchicalAttentionModule(nn.Module):
    """Hierarchical attention for anatomical structure hierarchy."""
    
    def __init__(self, in_channels: int, config: AttentionConfig):
        super().__init__()
        self.config = config
        self.hierarchy_levels = config.hierarchy_levels
        
        # Hierarchical attention layers
        self.hierarchy_attentions = nn.ModuleList([
            nn.Sequential(
                nn.Conv3d(in_channels, in_channels // 4, kernel_size=1, bias=False),
                nn.BatchNorm3d(in_channels // 4),
                nn.ReLU(inplace=True),
                nn.Conv3d(in_channels // 4, 1, kernel_size=1, bias=False),
                nn.Sigmoid()
            ) for _ in range(self.hierarchy_levels)
        ])
        
        # Level fusion
        self.level_fusion = nn.Sequential(
            nn.Conv3d(self.hierarchy_levels, 1, kernel_size=1, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor, hierarchy_masks: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Apply hierarchical attention.
        
        Args:
            x: Input features [B, C, H, W, D]
            hierarchy_masks: Optional hierarchy guidance [B, levels, H, W, D]
            
        Returns:
            Hierarchically attended features [B, C, H, W, D]
        """
        level_attentions = []
        
        for level, attention_layer in enumerate(self.hierarchy_attentions):
            level_attention = attention_layer(x)  # [B, 1, H, W, D]
            
            # Apply hierarchy mask if provided
            if hierarchy_masks is not None and level < hierarchy_masks.shape[1]:
                hierarchy_mask = hierarchy_masks[:, level:level+1, :, :, :]  # [B, 1, H, W, D]
                level_attention = level_attention * hierarchy_mask
            
            level_attentions.append(level_attention)
        
        # Combine hierarchical levels
        combined_levels = torch.cat(level_attentions, dim=1)  # [B, levels, H, W, D]
        final_attention = self.level_fusion(combined_levels)  # [B, 1, H, W, D]
        
        attended_x = x * final_attention
        
        return attended_x


class MultiScaleAttentionBlock(nn.Module):
    """Complete multi-scale attention block combining all attention mechanisms."""
    
    def __init__(self, in_channels: int, config: AttentionConfig = None):
        super().__init__()
        self.config = config or AttentionConfig()
        
        # Attention modules
        self.spatial_attention = SpatialAttentionModule(in_channels, self.config)
        self.channel_attention = ChannelAttentionModule(in_channels, self.config)
        self.morphogen_attention = MorphogenGuidedAttention(in_channels, self.config)
        self.hierarchical_attention = HierarchicalAttentionModule(in_channels, self.config)
        
        # Residual connections
        self.residual_weight = nn.Parameter(torch.ones(4))  # Learnable weights for each attention type
        
        logger.info(f"Initialized MultiScaleAttentionBlock with {in_channels} channels")
        
    def forward(self, x: torch.Tensor, morphogens: Optional[torch.Tensor] = None,
                hierarchy_masks: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Apply complete multi-scale attention.
        
        Args:
            x: Input features [B, C, H, W, D]
            morphogens: Optional morphogen gradients [B, 3, H, W, D]
            hierarchy_masks: Optional hierarchy guidance [B, levels, H, W, D]
            
        Returns:
            Multi-scale attended features [B, C, H, W, D]
        """
        residual = x
        
        # Apply different attention mechanisms
        attended_features = []
        
        # Spatial attention
        spatial_out = self.spatial_attention(x)
        attended_features.append(spatial_out)
        
        # Channel attention
        channel_out = self.channel_attention(x)
        attended_features.append(channel_out)
        
        # Morphogen attention (if available)
        if morphogens is not None:
            morphogen_out = self.morphogen_attention(x, morphogens)
            attended_features.append(morphogen_out)
        else:
            attended_features.append(x)  # Identity if no morphogens
        
        # Hierarchical attention
        hierarchical_out = self.hierarchical_attention(x, hierarchy_masks)
        attended_features.append(hierarchical_out)
        
        # Weighted combination
        weights = F.softmax(self.residual_weight, dim=0)
        combined = sum(w * feat for w, feat in zip(weights, attended_features))
        
        # Residual connection
        output = residual + combined
        
        return output


class AnatomicalComplexityHandler(nn.Module):
    """Main handler for anatomical complexity using multi-scale attention."""
    
    def __init__(self, feature_channels: List[int], config: AttentionConfig = None):
        super().__init__()
        self.config = config or AttentionConfig()
        self.feature_channels = feature_channels
        
        # Multi-scale attention blocks for different feature levels
        self.attention_blocks = nn.ModuleList([
            MultiScaleAttentionBlock(channels, self.config)
            for channels in feature_channels
        ])
        
        # Cross-scale fusion
        self.cross_scale_fusion = self._build_cross_scale_fusion()
        
        logger.info(f"Initialized AnatomicalComplexityHandler with {len(feature_channels)} scales")
        
    def _build_cross_scale_fusion(self) -> nn.Module:
        """Build cross-scale feature fusion module."""
        
        total_channels = sum(self.feature_channels)
        
        return nn.Sequential(
            nn.Conv3d(total_channels, total_channels // 2, kernel_size=1, bias=False),
            nn.BatchNorm3d(total_channels // 2),
            nn.ReLU(inplace=True),
            nn.Conv3d(total_channels // 2, total_channels // 4, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm3d(total_channels // 4),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, multi_scale_features: List[torch.Tensor],
                morphogens: Optional[torch.Tensor] = None,
                hierarchy_masks: Optional[torch.Tensor] = None) -> List[torch.Tensor]:
        """
        Process multi-scale features with anatomical complexity handling.
        
        Args:
            multi_scale_features: List of feature tensors at different scales
            morphogens: Optional morphogen gradients [B, 3, H, W, D]
            hierarchy_masks: Optional hierarchy guidance [B, levels, H, W, D]
            
        Returns:
            List of attention-enhanced features at each scale
        """
        attended_features = []
        
        # Apply attention to each scale
        for i, (features, attention_block) in enumerate(zip(multi_scale_features, self.attention_blocks)):
            # Resize morphogens and hierarchy masks to match feature scale
            scale_morphogens = None
            scale_hierarchy = None
            
            if morphogens is not None:
                scale_morphogens = F.interpolate(
                    morphogens, size=features.shape[2:], mode='trilinear', align_corners=False
                )
            
            if hierarchy_masks is not None:
                scale_hierarchy = F.interpolate(
                    hierarchy_masks, size=features.shape[2:], mode='trilinear', align_corners=False
                )
            
            # Apply attention
            attended = attention_block(features, scale_morphogens, scale_hierarchy)
            attended_features.append(attended)
        
        return attended_features


def create_anatomical_complexity_model(base_model: nn.Module, 
                                     feature_channels: List[int],
                                     config: AttentionConfig = None) -> nn.Module:
    """
    Enhance base model with anatomical complexity handling.
    
    Args:
        base_model: Base segmentation model
        feature_channels: List of feature channel counts at different scales
        config: Attention configuration
        
    Returns:
        Enhanced model with multi-scale attention
    """
    
    class AnatomicallyEnhancedModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.base_model = base_model
            self.complexity_handler = AnatomicalComplexityHandler(feature_channels, config)
            
        def forward(self, x, morphogens=None, hierarchy_masks=None):
            # Extract multi-scale features from base model
            if hasattr(self.base_model, 'extract_features'):
                multi_scale_features = self.base_model.extract_features(x)
            else:
                # Fallback: use base model as single-scale
                base_features = self.base_model(x)
                multi_scale_features = [base_features]
            
            # Apply anatomical complexity handling
            enhanced_features = self.complexity_handler(
                multi_scale_features, morphogens, hierarchy_masks
            )
            
            # Final prediction (use the highest resolution features)
            if hasattr(self.base_model, 'final_prediction'):
                output = self.base_model.final_prediction(enhanced_features[-1])
            else:
                # Simple final layer
                final_conv = nn.Conv3d(enhanced_features[-1].shape[1], 6, kernel_size=1)
                output = final_conv(enhanced_features[-1])
            
            return output
    
    return AnatomicallyEnhancedModel()


def main():
    """Test multi-scale attention modules."""
    
    print("üß† MULTI-SCALE ATTENTION - Anatomical Complexity Mitigation")
    print("=" * 60)
    
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    
    # Create test configuration
    config = AttentionConfig(
        spatial_scales=[1, 2, 4],
        spatial_reduction_ratio=8,
        channel_reduction_ratio=16,
        morphogen_channels=3,
        morphogen_weight=0.3,
        hierarchy_levels=3
    )
    
    # Test dimensions
    batch_size = 2
    in_channels = 64
    H, W, D = 64, 64, 48
    
    # Create test data
    print("Creating test data...")
    x = torch.randn(batch_size, in_channels, H, W, D)
    morphogens = torch.randn(batch_size, 3, H, W, D)
    hierarchy_masks = torch.randn(batch_size, 3, H, W, D)
    
    print(f"Input shape: {x.shape}")
    print(f"Morphogens shape: {morphogens.shape}")
    print(f"Hierarchy masks shape: {hierarchy_masks.shape}")
    
    # Test individual attention modules
    print("\nüîç Testing individual attention modules...")
    
    # Spatial attention
    spatial_attention = SpatialAttentionModule(in_channels, config)
    spatial_out = spatial_attention(x)
    print(f"‚úÖ Spatial attention output: {spatial_out.shape}")
    
    # Channel attention
    channel_attention = ChannelAttentionModule(in_channels, config)
    channel_out = channel_attention(x)
    print(f"‚úÖ Channel attention output: {channel_out.shape}")
    
    # Morphogen attention
    morphogen_attention = MorphogenGuidedAttention(in_channels, config)
    morphogen_out = morphogen_attention(x, morphogens)
    print(f"‚úÖ Morphogen attention output: {morphogen_out.shape}")
    
    # Hierarchical attention
    hierarchical_attention = HierarchicalAttentionModule(in_channels, config)
    hierarchical_out = hierarchical_attention(x, hierarchy_masks)
    print(f"‚úÖ Hierarchical attention output: {hierarchical_out.shape}")
    
    # Test complete multi-scale attention block
    print("\nüîó Testing complete multi-scale attention block...")
    attention_block = MultiScaleAttentionBlock(in_channels, config)
    
    # Forward pass
    with torch.no_grad():
        output = attention_block(x, morphogens, hierarchy_masks)
    
    print(f"‚úÖ Multi-scale attention output: {output.shape}")
    
    # Test anatomical complexity handler
    print("\nüèóÔ∏è Testing anatomical complexity handler...")
    feature_channels = [32, 64, 128, 256]
    multi_scale_features = [
        torch.randn(batch_size, channels, H//(2**i), W//(2**i), D//(2**i))
        for i, channels in enumerate(feature_channels)
    ]
    
    complexity_handler = AnatomicalComplexityHandler(feature_channels, config)
    
    with torch.no_grad():
        enhanced_features = complexity_handler(multi_scale_features, morphogens, hierarchy_masks)
    
    print(f"‚úÖ Enhanced features: {[f.shape for f in enhanced_features]}")
    
    # Performance metrics
    print("\nüìä PERFORMANCE METRICS")
    
    # Parameter count
    total_params = sum(p.numel() for p in attention_block.parameters())
    print(f"   Multi-scale attention parameters: {total_params:,}")
    
    # Memory usage (approximate)
    input_memory = x.numel() * 4 / (1024**2)  # MB (float32)
    output_memory = output.numel() * 4 / (1024**2)  # MB
    print(f"   Input memory: {input_memory:.1f} MB")
    print(f"   Output memory: {output_memory:.1f} MB")
    
    # Computational complexity (FLOPs estimate)
    flops_estimate = total_params * H * W * D / 1e9  # Rough estimate in GFLOPs
    print(f"   Estimated FLOPs: {flops_estimate:.2f} GFLOPs")
    
    print(f"\n‚úÖ Multi-scale attention system ready!")
    print(f"   Anatomical complexity risk: MITIGATED")
    print(f"   Features: Multi-scale spatial, channel, morphogen, and hierarchical attention")
    print(f"   Integration: Ready for brainstem segmentation model")
    
    return attention_block, complexity_handler


if __name__ == "__main__":
    attention_block, complexity_handler = main()
