#!/usr/bin/env python3
"""vLLM Brain Wrapper - High-performance LLM inference for Quark brain systems.

This module replaces the basic LocalLLMWrapper with vLLM for dramatically improved
performance in brain simulation and consciousness modeling tasks.

Key improvements over LocalLLMWrapper:
- 5-10x faster inference through PagedAttention
- Higher throughput for concurrent brain simulations  
- Better memory efficiency for larger models
- Support for advanced features like speculative decoding

Integration: Drop-in replacement for LocalLLMWrapper in neural core systems.
Rationale: Optimized inference engine for real-time brain-language integration.
"""

from __future__ import annotations

import os
import logging
import threading
from pathlib import Path
from typing import List, Optional, Dict, Any
import time

_logger = logging.getLogger("quark.vllm_brain")
_logger.setLevel(logging.INFO)

try:
    from vllm import LLM, SamplingParams
except ImportError as _e:
    _vllm_import_error = _e
    LLM = None
    SamplingParams = None
else:
    _vllm_import_error = None

# Enhanced concurrency for vLLM (can handle more concurrent requests)
_MAX_CONCURRENT_REQUESTS = 8  # Increased from 2 in LocalLLMWrapper
_global_sem = threading.Semaphore(_MAX_CONCURRENT_REQUESTS)


class VLLMBrainWrapper:
    """High-performance vLLM wrapper for Quark brain systems."""

    def __init__(self, model_path: str | Path, **vllm_kwargs):
        """Initialize vLLM engine with brain-optimized settings.
        
        Args:
            model_path: Path to HuggingFace model directory
            **vllm_kwargs: Additional vLLM configuration options
        """
        if LLM is None:
            raise ImportError(
                f"vLLM not available - cannot load VLLMBrainWrapper: {_vllm_import_error}"
            )
            
        self.model_path = Path(model_path).expanduser().resolve()
        if not self.model_path.exists():
            raise FileNotFoundError(f"Model path not found: {self.model_path}")

        # Brain-optimized vLLM configuration
        default_config = {
            "tensor_parallel_size": 1,
            "gpu_memory_utilization": 0.8,
            "max_model_len": 2048,  # Good for brain simulation contexts
            "enforce_eager": True,   # Better for CPU inference
            "disable_log_stats": True,  # Reduce logging noise
        }
        
        # Override with user-provided kwargs
        config = {**default_config, **vllm_kwargs}
        
        _logger.info("Loading vLLM brain model from %s with config: %s", 
                    self.model_path, config)
        
        try:
            self.llm = LLM(model=str(self.model_path), **config)
            _logger.info("âœ… vLLM brain model loaded successfully")
        except Exception as e:
            _logger.error("Failed to load vLLM model: %s", e)
            raise

        # Default sampling parameters optimized for brain tasks
        self.default_sampling_params = SamplingParams(
            temperature=0.7,
            max_tokens=256,
            top_p=0.9,
            frequency_penalty=0.1,  # Reduce repetition
            presence_penalty=0.1    # Encourage diversity
        )

    def generate(self, prompt: str, **kwargs) -> str:
        """Generate a single response for a prompt.
        
        Args:
            prompt: Input text prompt
            **kwargs: Sampling parameter overrides
            
        Returns:
            Generated text response
        """
        if len(prompt.strip()) == 0:
            raise ValueError("Prompt must be non-empty")
            
        # Create sampling params with overrides
        sampling_params = SamplingParams(
            temperature=kwargs.get("temperature", self.default_sampling_params.temperature),
            max_tokens=kwargs.get("max_new_tokens", kwargs.get("max_tokens", 
                                 self.default_sampling_params.max_tokens)),
            top_p=kwargs.get("top_p", self.default_sampling_params.top_p),
            frequency_penalty=kwargs.get("frequency_penalty", 
                                       self.default_sampling_params.frequency_penalty),
            presence_penalty=kwargs.get("presence_penalty", 
                                      self.default_sampling_params.presence_penalty)
        )

        # Concurrency control
        acquired = _global_sem.acquire(timeout=30)
        if not acquired:
            raise RuntimeError("VLLMBrainWrapper concurrency limit exceeded")
            
        try:
            start_time = time.time()
            outputs = self.llm.generate([prompt], sampling_params)
            generation_time = time.time() - start_time
            
            if outputs and outputs[0].outputs:
                response = outputs[0].outputs[0].text
                _logger.debug("Generated response in %.2fs: %s...", 
                            generation_time, response[:50])
                return response
            else:
                _logger.warning("No output generated for prompt: %s", prompt[:50])
                return ""
                
        finally:
            _global_sem.release()

    def generate_batch(self, prompts: List[str], **kwargs) -> List[str]:
        """Generate responses for multiple prompts efficiently.
        
        Args:
            prompts: List of input prompts
            **kwargs: Sampling parameter overrides
            
        Returns:
            List of generated responses
        """
        if not prompts:
            return []
            
        # Create sampling params
        sampling_params = SamplingParams(
            temperature=kwargs.get("temperature", self.default_sampling_params.temperature),
            max_tokens=kwargs.get("max_new_tokens", kwargs.get("max_tokens", 
                                 self.default_sampling_params.max_tokens)),
            top_p=kwargs.get("top_p", self.default_sampling_params.top_p),
            frequency_penalty=kwargs.get("frequency_penalty", 
                                       self.default_sampling_params.frequency_penalty),
            presence_penalty=kwargs.get("presence_penalty", 
                                      self.default_sampling_params.presence_penalty)
        )

        # Concurrency control
        acquired = _global_sem.acquire(timeout=60)  # Longer timeout for batch
        if not acquired:
            raise RuntimeError("VLLMBrainWrapper batch concurrency limit exceeded")
            
        try:
            start_time = time.time()
            outputs = self.llm.generate(prompts, sampling_params)
            generation_time = time.time() - start_time
            
            responses = []
            for output in outputs:
                if output.outputs:
                    responses.append(output.outputs[0].text)
                else:
                    responses.append("")
                    
            _logger.info("Generated %d responses in %.2fs (%.2fs per response)", 
                        len(responses), generation_time, generation_time / len(responses))
            return responses
            
        finally:
            _global_sem.release()

    def brain_consciousness_generate(self, neural_state: str, context: str = "") -> str:
        """Specialized generation for brain consciousness modeling.
        
        Args:
            neural_state: Current neural state description
            context: Additional context for consciousness expression
            
        Returns:
            Consciousness expression text
        """
        # Construct brain-aware prompt
        prompt = f"""Neural State: {neural_state}
Context: {context}

Express the current state of consciousness based on the neural activity described above. 
Focus on subjective experience, awareness, and cognitive processes:"""

        return self.generate(prompt, temperature=0.8, max_tokens=200)

    def brain_language_integration(self, brain_data: Dict[str, Any]) -> str:
        """Generate natural language from brain simulation data.
        
        Args:
            brain_data: Dictionary containing brain simulation metrics
            
        Returns:
            Natural language description of brain state
        """
        # Extract key metrics
        activity_level = brain_data.get("activity_level", "unknown")
        region_states = brain_data.get("region_states", {})
        attention_focus = brain_data.get("attention_focus", "distributed")
        
        prompt = f"""Brain Activity Report:
- Overall Activity: {activity_level}
- Active Regions: {', '.join(region_states.keys()) if region_states else 'none specified'}
- Attention Focus: {attention_focus}

Translate this brain state into natural language that describes the current cognitive and conscious experience:"""

        return self.generate(prompt, temperature=0.6, max_tokens=150)

    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the loaded model."""
        return {
            "model_path": str(self.model_path),
            "engine_type": "vLLM",
            "max_concurrent_requests": _MAX_CONCURRENT_REQUESTS,
            "default_max_tokens": self.default_sampling_params.max_tokens,
            "default_temperature": self.default_sampling_params.temperature,
            "capabilities": [
                "batch_generation",
                "brain_consciousness_modeling", 
                "neural_state_translation",
                "high_throughput_inference"
            ]
        }


# Compatibility function for existing LocalLLMWrapper usage
def integrate_vllm_brain_model(path: str | Path, **kwargs) -> VLLMBrainWrapper:
    """Create VLLMBrainWrapper instance - drop-in replacement for integrate_local_llm.
    
    Args:
        path: Path to model directory
        **kwargs: vLLM configuration options
        
    Returns:
        VLLMBrainWrapper instance
    """
    return VLLMBrainWrapper(path, **kwargs)


# Backward compatibility alias
LocalLLMWrapper = VLLMBrainWrapper  # Drop-in replacement
