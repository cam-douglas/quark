"""
Multi-Model Trainer for SmallMind

Trains all 3 models simultaneously with proper error handling
and tokenizer compatibility fixes.
"""

import os, sys
import time
import signal
import logging
import json
import threading
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import subprocess

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """Configuration for a single model"""
    name: str
    path: str
    memory_gb: float
    batch_size: int
    gradient_accumulation: int
    learning_rate: float
    max_steps: int
    output_dir: str
    status: str = "pending"  # pending, running, completed, failed, error

class MultiModelTrainer:
    """Trains multiple models simultaneously"""
    
    def __init__(self, base_output_dir: str = "./multi_model_training"):
        self.base_output_dir = Path(base_output_dir)
        self.base_output_dir.mkdir(parents=True, exist_ok=True)
        
        # Model configurations
        self.models = self._setup_models()
        
        # Training state
        self.should_stop = False
        self.training_threads = {}
        self.training_status = {}
        
        # Setup signal handlers
        self._setup_signal_handlers()
        
        logger.info(f"MultiModelTrainer initialized with output_dir: {self.base_output_dir}")
    
    def _setup_signal_handlers(self):
        """Setup signal handlers for graceful shutdown"""
        def signal_handler(signum, frame):
            logger.info(f"Received signal {signum}, stopping all training...")
            self.should_stop = True
            self.stop_all_training()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    def _setup_models(self) -> List[ModelConfig]:
        """Setup configurations for all available models"""
        models = []
        
        # DeepSeek V2 - High performance, high memory
        deepseek_path = "src/smallmind/models/models/checkpoints/deepseek-v2"
        if os.path.exists(deepseek_path):
            models.append(ModelConfig(
                name="DeepSeek-V2",
                path=deepseek_path,
                memory_gb=32.0,
                batch_size=1,  # Reduced due to memory constraints
                gradient_accumulation=8,
                learning_rate=3e-5,
                max_steps=1000,
                output_dir=str(self.base_output_dir / "deepseek_v2")
            ))
        
        # Qwen 1.5 MoE - Balanced performance, moderate memory
        qwen_path = "src/smallmind/models/models/checkpoints/qwen1.5-moe"
        if os.path.exists(qwen_path):
            models.append(ModelConfig(
                name="Qwen1.5-MoE",
                path=qwen_path,
                memory_gb=8.0,
                batch_size=4,
                gradient_accumulation=4,
                learning_rate=5e-5,
                max_steps=1000,
                output_dir=str(self.base_output_dir / "qwen_moe")
            ))
        
        # MixTAO MoE - Balanced performance, moderate memory
        mixtao_path = "src/smallmind/models/models/checkpoints/mix-tao-moe"
        if os.path.exists(mixtao_path):
            models.append(ModelConfig(
                name="MixTAO-MoE",
                path=mixtao_path,
                memory_gb=16.0,
                batch_size=2,
                gradient_accumulation=4,
                learning_rate=5e-5,
                max_steps=1000,
                output_dir=str(self.base_output_dir / "mixtao_moe")
            ))
        
        if not models:
            logger.warning("No models found! Check your model paths.")
        
        return models
    
    def _create_training_script(self, model: ModelConfig) -> str:
        """Create a training script for a specific model"""
        script_content = f'''#!/usr/bin/env python3
"""
Training script for {model.name}
Generated by MultiModelTrainer
"""

import os, sys
import logging
from pathlib import Path

# Add the src directory to the path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - {model.name} - %(levelname)s - %(message)s'
)

try:
    from smallmind.models.continuous_trainer import ContinuousTrainer
    
    # Create trainer with model-specific settings
    trainer = ContinuousTrainer(
        model_path="{model.path}",
        output_dir="{model.output_dir}"
    )
    
    # Override default config for this model
    trainer.config = type('Config', (), {{
        'steps_per_epoch': {model.max_steps},
        'batch_size': {model.batch_size},
        'learning_rate': {model.learning_rate}
    }})()
    
    print(f"üöÄ Starting training for {{model.name}}")
    print(f"   Model: {{model.path}}")
    print(f"   Output: {{model.output_dir}}")
    print(f"   Batch size: {{model.batch_size}}")
    print(f"   Learning rate: {{model.learning_rate}}")
    
    # Start training
    trainer.train_forever()
    
except Exception as e:
    print(f"‚ùå Training failed for {{model.name}}: {{e}}")
    sys.exit(1)
'''
        
        # Write script to file
        script_path = self.base_output_dir / f"train_{model.name.lower().replace('-', '_').replace('.', '_')}.py"
        with open(script_path, 'w') as f:
            f.write(script_content)
        
        # Make executable
        os.chmod(script_path, 0o755)
        
        return str(script_path)
    
    def _train_model(self, model: ModelConfig):
        """Train a single model in a separate process"""
        try:
            logger.info(f"üöÄ Starting training for {model.name}")
            model.status = "running"
            
            # Create training script
            script_path = self._create_training_script(model)
            
            # Start training process
            process = subprocess.Popen(
                [sys.executable, script_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                cwd=os.getcwd()
            )
            
            # Store process reference
            self.training_threads[model.name] = process
            
            # Monitor process
            stdout, stderr = process.communicate()
            
            if process.returncode == 0:
                model.status = "completed"
                logger.info(f"‚úÖ {model.name} training completed successfully")
            else:
                model.status = "failed"
                logger.error(f"‚ùå {model.name} training failed with return code {process.returncode}")
                if stderr:
                    logger.error(f"Error output: {stderr}")
            
        except Exception as e:
            model.status = "error"
            logger.error(f"‚ùå Error training {model.name}: {e}")
    
    def start_all_training(self):
        """Start training for all models simultaneously"""
        if not self.models:
            logger.error("No models available for training")
            return
        
        logger.info(f"üöÄ Starting simultaneous training for {len(self.models)} models")
        logger.info("Press Ctrl+C to stop all training gracefully")
        
        # Create output directories
        for model in self.models:
            Path(model.output_dir).mkdir(parents=True, exist_ok=True)
        
        # Start training threads for each model
        threads = []
        for model in self.models:
            thread = threading.Thread(
                target=self._train_model,
                args=(model,),
                name=f"trainer-{model.name}"
            )
            thread.daemon = True
            thread.start()
            threads.append(thread)
            
            # Small delay between starts to avoid resource conflicts
            time.sleep(2)
        
        # Wait for all threads to complete or be interrupted
        try:
            while any(t.is_alive() for t in threads) and not self.should_stop:
                time.sleep(5)
                self._print_status()
                
        except KeyboardInterrupt:
            logger.info("Received interrupt, stopping all training...")
            self.should_stop = True
            self.stop_all_training()
        
        # Final status
        self._print_final_status()
    
    def stop_all_training(self):
        """Stop all running training processes"""
        logger.info("üõë Stopping all training processes...")
        
        for model_name, process in self.training_threads.items():
            if process.poll() is None:  # Process is still running
                logger.info(f"Stopping {model_name}...")
                process.terminate()
                
                # Wait a bit for graceful shutdown
                try:
                    process.wait(timeout=30)
                except subprocess.TimeoutExpired:
                    logger.warning(f"Force killing {model_name}")
                    process.kill()
        
        logger.info("All training processes stopped")
    
    def _print_status(self):
        """Print current training status"""
        print("\n" + "="*60)
        print(f"üìä Training Status - {datetime.now().strftime('%H:%M:%S')}")
        print("="*60)
        
        for model in self.models:
            status_emoji = {
                "pending": "‚è≥",
                "running": "üîÑ",
                "completed": "‚úÖ",
                "failed": "‚ùå",
                "error": "üí•"
            }.get(model.status, "‚ùì")
            
            print(f"{status_emoji} {model.name}: {model.status}")
            if model.status == "running":
                print(f"   üìÅ Output: {model.output_dir}")
                print(f"   üß† Memory: {model.memory_gb}GB")
                print(f"   üì¶ Batch: {model.batch_size} (x{model.gradient_accumulation})")
        
        print("="*60)
    
    def _print_final_status(self):
        """Print final training status"""
        print("\n" + "üéØ"*20)
        print("üèÅ FINAL TRAINING STATUS")
        print("üéØ"*20)
        
        completed = sum(1 for m in self.models if m.status == "completed")
        failed = sum(1 for m in self.models if m.status in ["failed", "error"])
        
        print(f"‚úÖ Completed: {completed}")
        print(f"‚ùå Failed: {failed}")
        print(f"üìä Total: {len(self.models)}")
        
        for model in self.models:
            status_emoji = {
                "pending": "‚è≥",
                "running": "üîÑ",
                "completed": "‚úÖ",
                "failed": "‚ùå",
                "error": "üí•"
            }.get(model.status, "‚ùì")
            
            print(f"{status_emoji} {model.name}: {model.status}")
            if model.status == "completed":
                print(f"   üìÅ Results saved to: {model.output_dir}")
        
        print("üéØ"*20)
    
    def get_status_summary(self) -> Dict[str, Any]:
        """Get a summary of training status"""
        return {
            "total_models": len(self.models),
            "completed": sum(1 for m in self.models if m.status == "completed"),
            "running": sum(1 for m in self.models if m.status == "running"),
            "failed": sum(1 for m in self.models if m.status in ["failed", "error"]),
            "pending": sum(1 for m in self.models if m.status == "pending"),
            "models": [
                {
                    "name": m.name,
                    "status": m.status,
                    "output_dir": m.output_dir,
                    "memory_gb": m.memory_gb
                }
                for m in self.models
            ]
        }

def start_multi_model_training(output_dir: str = "./multi_model_training"):
    """Start training all models simultaneously"""
    trainer = MultiModelTrainer(output_dir)
    trainer.start_all_training()
    return trainer

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Train all SmallMind models simultaneously")
    parser.add_argument("--output-dir", default="./multi_model_training", help="Output directory")
    
    args = parser.parse_args()
    
    print("üöÄ SmallMind Multi-Model Training")
    print("=" * 50)
    print("This will train ALL available models simultaneously")
    print("Press Ctrl+C to stop all training gracefully")
    print("=" * 50)
    
    try:
        trainer = start_multi_model_training(args.output_dir)
    except KeyboardInterrupt:
        print("\n\nüõë Multi-model training stopped by user")
    except Exception as e:
        print(f"\n‚ùå Multi-model training failed: {e}")
