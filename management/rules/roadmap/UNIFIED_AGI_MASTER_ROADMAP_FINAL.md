# AGI System Design: Main Rules for Cursor (Expanded)

This file defines the **core principles, modules, and validated constraints** for building an AGI using modular orchestration, memory systems, planning, and self-improvement. Everything here is backed by peer-reviewed or open research as of 2025.

---

## PHASE 1: FOUNDATION LAYER

### 1. Context Management Service
- Employ **persistent, queryable knowledge graphs** (KGs) with versioning and real-time updates, compatible with retrieval-augmented generation (RAG) patterns.
- Best practice: integrate vector DBs and symbolic nodes‚Äîe.g., via systems like LangGraph + Neo4j.

### 2. Memory Service
- Split into:
  - **Episodic memory**: temporal interactions (e.g., EM‚ÄëLLM for infinite context & event segmentation).
  - **Semantic memory**: structured facts and patterns.
- Uses learned consolidation policies (short-term ‚Üí long-term).
- Enhance long-context recall with memory‚Äëaugmented techniques like MemReasoner and MemLong. Benchmark via MemoryAgentBench for retrieval, test‚Äëtime learning, long‚Äërange understanding, and conflict resolution.

### 3. Workflow Engine
- Deterministically orchestrate probabilistic modules with rollback and checkpointing (inspired by SagaLLM‚Äôs context and transaction guarantees).

### 4. Agent Coordination Layer
- Multi-agent setup with negotiated consensus and role-based task routing.
- Use frameworks like ALAS for stateful, disruption-aware planning to handle failure gracefully.

---

## PHASE 2: CAPABILITY LAYER

### 1. Specialized Model Controls
- Modular, fine-tuned domain experts for tasks like text-to-text, image, code, symbolic reasoning etc.
- Use plug-and-play APIs, possibly external models such as DeepSeek R1, Claude 3.7, GPT‚Äë4o.

### 2. Symbolic Reasoning Engine
- Hybrid neural-symbolic integration.
- Incorporate calculators, compilers, solvers.
- Planning support by LLM‚ÄëRegress (lifted regression with LLM‚Äëinferred affordances).

### 3. Planning & Goal Management
- Decompose complex goals into subplans with subgoal generation, criteria, and dependencies.
- Use HyperTree Planning (HTP) for hierarchical reasoning via hypertrees; 3.6√ó travel planning improvement using Gemini‚Äë1.5‚ÄëPro.
- Leverage external module‚Äìaugmented planning or classical planners formalized via LLMs.

### 4. Cross-Modal Integration
- Build unified embeddings across text, vision, audio, video etc.
- On-device systems like Reminisce offer efficient, early-exiting multimodal memory embeddings.

---

## SYSTEM BEHAVIOR

### Orchestration Logic
- **Conductor LLM** decomposes tasks, routes to domain experts, and coordinates verification.

### Verification Layer
- Each expert paired with a verifier: rule-based, compiler, classifier, or LLM.

### Knowledge Gap Identification
- Detect gaps extrinsically via user feedback or intrinsically via Oracle LLM self-critique.

### Self-Evolution Protocol
- Architect LLM sources new tools or trains new experts (e.g., Hugging Face, Github models etc).

---

## SYSTEM INFRASTRUCTURE

### Cloud & Quantum Integration
- Integrate with **AWS cloud infrastructure**, especially:
  - Automatic access to the user's S3 bucket for:
    - Persistent context storage
    - Memory snapshots and rollback logs
    - Dataset caching and module state tracking
  - Dynamically create new S3 buckets when task demands exceed default capacity
  - Smart load balancing across EC2/Spot/On-Demand instances depending on priority and cost
- Use **AWS Braket quantum computing** only when needed for:
  - Complex optimization problems (e.g., hyperparameter tuning, symbolic graph search)
  - Probabilistic sampling tasks where quantum advantage is measurable
  - AGI modules flagged for combinatorially hard problems (with fallback to classical solvers)
- Deploy in **hybrid cloud + edge modes**, enabling local inference for low-latency tasks and cloud-scale capacity for training, simulation, and coordination


### Hardware-Agnostic Logic Execution
- Convert abstract logic into arithmetic flows, enabling portability across CPUs/GPUs/TPUs/FPGAs 

### Global Context Sharing
- Shared memory via DisNet-like servers to synchronize context across modules and devices.

---

## ADDITIONAL CRITICAL CONSIDERATIONS

### 1. Security & Sandboxing
- Sandbox new tools and model downloads; defend against LLM leakage via model inversion or membership inference attacks.

### 2. Observability & Logging
- Ensure full logging of agent decisions, context states, verifications, and rollbacks for auditing and debugging.

### 3. Error Propagation & Resilience
- Use frameworks like ALAS and SagaLLM for local compensation protocols and transactional consistency.

### 4. Reward & Success Modeling
- Integrate explicit reward mechanisms or scoring functions to evaluate successful module outputs (e.g., verified chains, goal achievement metrics from planning benchmarks).

### 5. Explainability & Introspection
- Provide mechanisms for module-level rationales‚Äîe.g., LLM-generated reasoning traces, symbolic logs, hierarchical hypertree debug views.

---

## MODULE MAP 

- **Architect**: Directs evolution and high‚Äëlevel design.
- **Conductor**: Plans, delegates, coordinates verification.
- **Oracle**: Reflects and critiques.
- **Domain Experts**: Specialized LLM, ML, symbolic, interpreters.
- **Verification Experts**: Rule-based verifiers, interpreters, LLM critics.
- **External Experts**: APIs, token searchers, downloaders.

---



## FINAL NOTES

This document aligns with state-of-the-art open research (ICLR, ACL, arXiv, Nature) and leading modular frameworks, maintaining rigorous standards of security, explainability, and resilience. Version this file and review it at least quarterly.

---

# üß† UNIFIED AGI-BRAIN SIMULATION MASTER ROADMAP

## üîß SYSTEM ARCHITECTURE & AGI ORCHESTRATION





---

## üìÑ FROM: `AGI_INTEGRATED_ROADMAP.md`

‚ö†Ô∏è File not found.

---

## üìÑ FROM: `HIGH_LEVEL_ROADMAP.md`

‚ö†Ô∏è File not found.

---

## üìÑ FROM: `BIOLOGICAL_AGI_DEVELOPMENT_ROADMAP.md`

‚ö†Ô∏è File not found.

---

## üìÑ FROM: `QUARK_ROADMAP.md`

‚ö†Ô∏è File not found.