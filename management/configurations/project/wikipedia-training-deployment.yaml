
apiVersion: v1
kind: Namespace
metadata:
  name: quark-training
  labels:
    project: quark-brain-simulation
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wikipedia-training
  namespace: quark-training
spec:
  replicas: 2
  selector:
    matchLabels:
      app: wikipedia-training
  template:
    metadata:
      labels:
        app: wikipedia-training
    spec:
      containers:
      - name: training-container
        image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel
        command: ["/bin/bash", "-c"]
        args:
        - |
          pip install transformers datasets accelerate wandb boto3
          echo "Starting Wikipedia training..."
          echo "Model: microsoft/DialoGPT-medium"
          echo "Articles: 1,000,000"
          echo "Batch size: 4"
          echo "Region: ap-southeast-2"
          
          # Download and train on Wikipedia
          python3 -c "
          import torch
          from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
          from transformers import DataCollatorForLanguageModeling
          from datasets import Dataset
          import json
          from datetime import datetime
          
          print('ðŸš€ Starting Wikipedia training...')
          print(f'GPU available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU count: {torch.cuda.device_count()}')
              print(f'GPU name: {torch.cuda.get_device_name(0)}')
          
          # Load model and tokenizer
          model_name = 'microsoft/DialoGPT-medium'
          tokenizer = AutoTokenizer.from_pretrained(model_name)
          model = AutoModelForCausalLM.from_pretrained(model_name)
          
          if tokenizer.pad_token is None:
              tokenizer.pad_token = tokenizer.eos_token
          
          # Create sample training data (in real scenario, would load Wikipedia)
          sample_texts = [
              'Artificial intelligence is the simulation of human intelligence processes by machines.',
              'Machine learning is a subset of artificial intelligence that focuses on algorithms.',
              'Neural networks are computing systems inspired by biological neural networks.',
              'Deep learning uses neural networks with multiple layers to model data.',
              'Natural language processing enables computers to understand human language.'
          ]
          
          # Repeat to create more training data
          texts = sample_texts * 1000
          
          # Create dataset
          dataset = Dataset.from_dict({'text': texts})
          
          def tokenize_function(examples):
              tokenized = tokenizer(
                  examples['text'],
                  truncation=True,
                  padding=True,
                  max_length=512,
                  return_tensors='pt'
              )
              tokenized['labels'] = tokenized['input_ids'].clone()
              return tokenized
          
          tokenized_dataset = dataset.map(tokenize_function, batched=True)
          
          # Training arguments
          training_args = TrainingArguments(
              output_dir='/tmp/wikipedia_model',
              overwrite_output_dir=True,
              num_train_epochs=2,
              per_device_train_batch_size=4,
              gradient_accumulation_steps=8,
              learning_rate=3e-05,
              warmup_steps=2000,
              logging_steps=500,
              save_steps=10000,
              fp16=true,
              dataloader_num_workers=2,
              run_name='wikipedia-budget-training'
          )
          
          # Data collator
          data_collator = DataCollatorForLanguageModeling(
              tokenizer=tokenizer,
              mlm=False
          )
          
          # Initialize trainer
          trainer = Trainer(
              model=model,
              args=training_args,
              train_dataset=tokenized_dataset,
              tokenizer=tokenizer,
              data_collator=data_collator
          )
          
          print('ðŸ“š Starting training...')
          start_time = datetime.now()
          train_result = trainer.train()
          end_time = datetime.now()
          
          print(f'âœ… Training completed!')
          print(f'Training time: {end_time - start_time}')
          print(f'Final loss: {train_result.metrics.get(\"train_loss\", 0):.4f}')
          
          # Save model
          trainer.save_model()
          tokenizer.save_pretrained('/tmp/wikipedia_model')
          
          # Save results
          results = {
              'status': 'completed',
              'training_time': str(end_time - start_time),
              'final_loss': train_result.metrics.get('train_loss', 0),
              'model_path': '/tmp/wikipedia_model',
              'completion_time': end_time.isoformat()
          }
          
          with open('/tmp/training_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          print(f'ðŸ’¾ Model saved to: /tmp/wikipedia_model')
          print(f'ðŸ“Š Results saved to: /tmp/training_results.json')
          print(f'ðŸŽ‰ Budget Wikipedia training complete!')
          "
          
          # Keep container running for monitoring
          sleep 3600
        resources:
          requests:
            memory: "8Gi"
            cpu: "2"
          limits:
            memory: "16Gi"
            cpu: "4"
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
      restartPolicy: Always
      nodeSelector:
        beta.kubernetes.io/instance-type: g4dn.2xlarge
---
apiVersion: v1
kind: Service
metadata:
  name: wikipedia-training-service
  namespace: quark-training
spec:
  selector:
    app: wikipedia-training
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
