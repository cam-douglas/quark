# ðŸš€ Advanced Small-Mind Dependencies
# Comprehensive requirements for advanced optimization and simulation

# Core scientific computing
numpy>=1.26.0
scipy>=1.12.0
pandas>=2.2.0
matplotlib>=3.8.0

# Deep Learning & ML
torch>=2.3.0
torchvision>=0.18.0
torchaudio>=2.3.0
transformers>=4.40.0
scikit-learn>=1.4.0

# Advanced Optimization
optuna>=3.6.0
sam>=1.0.0  # Sharpness-Aware Minimization
ray>=2.10.0  # Distributed computing

# High-Performance ML
vllm>=0.5.0  # PagedAttention + continuous batching
flash-attn>=2.8.0  # FlashAttention-2 kernels
accelerate>=0.30.0  # FSDP, DeepSpeed integration
deepspeed>=0.14.0  # ZeRO-3, CPU/NVMe offload
bitsandbytes>=0.43.0  # 4-bit quantization
autoawq>=0.2.0  # 4-bit weight-only inference

# Physics Simulation
pybullet>=3.2.5
pymunk>=6.6.0
mujoco>=2.3.0

# Data Processing & Visualization
seaborn>=0.13.0
plotly>=5.20.0
pyvista>=0.44.0
vtk>=9.3.0

# Web & API
fastapi>=0.110.0
uvicorn[standard]>=0.30.0
streamlit>=1.32.0

# Development & Testing
pytest>=8.0.0
black>=24.0.0
ruff>=0.3.0

# Utilities
tqdm>=4.66.0
rich>=13.7.0
click>=8.1.0
pydantic>=2.6.0

# Optional: AWS & Cloud
boto3>=1.34.0
mlflow>=2.10.0
wandb>=0.17.0

# Optional: Additional ML Tools
xformers>=0.0.24  # Memory-efficient attention
optimum>=1.20.0  # ONNX, TensorRT integration
onnx>=1.16.0  # Model export optimization
onnxruntime-gpu>=1.17.0  # GPU-accelerated inference

# Optional: Quantization
smoothquant>=0.1.0  # INT8 activation quantization
gptq>=0.0.2  # 4-bit weight quantization
squeezellm>=0.1.0  # 3-bit quantization

# Optional: High-Performance Data
vaex>=4.17.0  # Out-of-core DataFrames
polars>=0.21.0  # Rust-based DataFrame
duckdb>=0.10.0  # In-memory analytical database
arrow>=15.0.0  # Columnar memory format

# Optional: Scientific Computing Optimization
mkl>=2024.0  # Intel Math Kernel Library
openblas>=0.3.26  # OpenBLAS optimizations
blis>=0.9.0  # BLAS implementation
